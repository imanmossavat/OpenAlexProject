# System Overview

This project ships three tightly-coupled applications plus shared assets so researchers can seed a crawl, orchestrate experiments, and interpret the "vault" outputs:

1. **Crawler core (`fakenewscitationnetwork/ArticleCrawler/`)** – Python package responsible for talking to OpenAlex, sampling citation networks, running topic modeling, and generating the vault artifacts.
2. **Backend API (`article-crawler-backend/`)** – FastAPI service that exposes crawler functionality over HTTP, handles background execution, persists sessions, and distributes vaults to users.
3. **Frontend (`frontend/`)** – Vite + React application implementing the multi-step workflow (seed → configure → run → analyze) and consuming the backend endpoints.

## How the Pieces Talk


1. Users interact entirely through the React UI. `shared/workflows/*` defines each wizard step which makes HTTP calls via `shared/api/client.js`.
2. FastAPI routers translate those calls into service invocations. Example: `POST /api/v1/crawler_execution/{session_id}/start` loads session data, builds crawler configs, and dispatches a background job.
3. `CrawlerExecutionService` uses `CrawlerJobRunner` to spin up the Python crawler. Progress snapshots flow back through a callback; the job store persists them so `/jobs/{job_id}/status` can respond instantly.
4. The crawler writes its vault output under a job-specific experiment folder. When the job completes, The frontend surfaces markdown summaries, tables, and figures from those files.
5. Optional dependencies (Dockerized GROBID, Zotero) plug into this flow through environment variables generated by `install.py` or `.env.docker.example`.

## Repository Map

| Path | Purpose | Linked Docs |
| ---- | ------- | ----------- |
| `fakenewscitationnetwork/ArticleCrawler/` | Core crawler implementation, CLI entrypoints, experiments/data folders. | `docs/components/crawler.md` |
| `article-crawler-backend/app/` | FastAPI routers + services (staging, crawler execution, seeds, PDFs, retraction). | `docs/components/backend.md` |
| `frontend/src/` | React sources (router, workflows, pages, shared components, assets). | `docs/components/frontend.md` |
| `docker/`, `docker-compose.yml` | Containerized workflow (API + frontend + GROBID). | `Installation guide.md` |
| `install.py` | Automated installer that provisions `.venv`, downloads dependencies, and writes `.env` files for all components. | `Installation guide.md` |


## Key Data Products ("Vault")

Every crawler run produces the same deliverables under `fakenewscitationnetwork/experiments/.../vault/`:

- `*_table_*.md` – Iteration tables summarizing seeds, citations, relevance scores.
- `summary/top_authors.md` & `summary/top_venues.md` – Aggregated leaderboards.
- `topics/topic_temporal_{lda,nmf}.md`, `topic_topwords_*`, `topic_wordcloud_*` – Topic modeling breakdowns (backed by PNGs in `figures/<timestamp>/`).
- `abstracts/*.md` or `papers/*.md` – One file per harvested paper.
- `run.md`, `README.md` – Machine-generated manifests covering runtime parameters, errors, and environment info.


## Job Lifecycle (End-to-End)

1. **Create session** – React wizard starts a session via `/api/v1/seed_sessions/start`. Backend stores JSON on disk.
2. **Seed data** – Seeds/keywords/config steps POST to `/seeds`, `/keywords`, `/configuration`. Backend validates inputs and persists them with the session.
3. **Launch** – `/crawler_execution/{session_id}/start` orchestrates `CrawlerConfigBuilder`, writes an experiment folder, and hands off to `CrawlerJobRunner`.
4. **Monitor** – Backend writes snapshots to the job store; frontend polls `/jobs/{job_id}/status` and renders metrics.
5. **Analyze** – Once `status=completed`, frontend hits `/jobs/{job_id}/results` and `/jobs/{job_id}/topics` to read vault metadata.
6. **Resume/iterate** – If users want to push the crawl further, `/jobs/{job_id}/resume` sends `ResumeState` data back into the crawler, optionally with manual paper IDs for the next frontier.

## Deployment Paths

- **Local full stack** – Run `python install.py` (root) to set up `.venv` + `.env`. Start backend with `uvicorn` and frontend with `npm run dev`. Launch optional GROBID container separately if PDF parsing is required.
- **Docker** – Copy `.env.docker.example` to `.env`, then `docker compose up --build`. The compose stack runs the installer in headless mode, starts FastAPI, the frontend, and a GROBID service, wiring them together on an internal network.

